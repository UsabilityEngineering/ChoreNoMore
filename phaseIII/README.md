# Phase III: Prototypes and User Testing

## Introduction

Chore No More is going to be a website that can support a wide range of users in scheduling chores with the members of their households. In this phase, our team built a prototype of the website which included all proposed functionality. Additionally, we ran a simulated user test of our prototype within our team and formal user testing on a few of our peers. We intend for this website to streamline and automate scheduling chores for families and roommates.

## Methods

We conducted a user test (n=5) with participants consisting of peers in our Usability Engineering class. Following their signing of an informed-consent form, we told participants to think aloud their thoughts as they went through the tasks we designed for them. We asked background questions to gauge participant familiarity with competing products, then we walked participants through 5 tasks, exploring various facets of the application:

  1. If the participant fell into the “young adult” demographic, we asked them to create an account and add a roommate to their household.
  2. We asked the participant to create a new task to be due next Tuesday.
  3. We asked the participant to mark a task as completed and another user’s task as uncompleted.
  4. We asked the user to check the calendar view to see which tasks were due next Thursday.
  5. We asked users to use the “cycle chores” functionality to randomly assign tasks between users in a household.

To avoid influencing participants of the study, we took care to word task prompts to avoid using any terminology present in the labeling on the application. After each task, we marked whether a participant completed the task and then asked them to rate its difficulty on a scale of 1 to 5, with 1 being “Very Difficult” and 5 being “Very Easy.” 

To find potential flaws with our methodology, we also conducted a simulated user test with one of the members of the UX team using the same procedure outlined above.

## Findings

When completing the simulated user test on our team prior to the full user testing we found a few issues:

* The navigation menu at the top of the screen would not always take the user to the correct view. This caused the user to not be able to complete their given tasks. 
* Sign-up took the user to Login rather than displaying the Home Page

These problems we found were then addressed before going into the user testing with our UX classmates.

Generally, the study participants had favorable ratings throughout the test, however, they identified several potential shortcomings of our designs:

1. While most users were able to complete the first task with little difficulty, some users briefly got stuck. One participant noted that they did not know the “Household” page would contain the ability to add a user. One found that the text on the top navigation bar was hard to read. Another expressed pleasure in the nomenclature used in the application: “I like that it says household and not members.” The average rating for Task 1 was 4.6 out of 5.
2. Most participants had no difficulties completing this task. However, one participant initially went into the “Manage Chores” menu, thinking they would be able to add a chore from there. After some struggle, they eventually found the “Add Chores” menu and successfully added a new chore. The average rating for Task 2 was 4.6 out of 5.
3. All users were able to complete this task without issues. However, one user expressed the desire to leave a comment to let another member of the household know why their task was marked as incomplete. The average rating for Task 3 was 5 out of 5.
5. All users were able to complete this task without issues. However, one user expressed confusion about the inability to click on a particular date to get an expanded view of the tasks. The average rating for Task 4 was 5 out of 5.
6. There was a consensus among participants that the name for the label to randomize chores, “Cycle Chores,” was confusing. Furthermore, several users expressed that the text was too small to easily read. One user thought chores would automatically be cycled by just entering the “Assign Chores” menu. The average rating for Task 5 was 4.2 out of 5.

When asked what they liked most about the application, we received recurring praise for the design of the application, with one user commenting, “I really like how clean the website is.” When asked how they would improve the application, two participants noted that they would reword the labeling on the “Cycle Chores” feature. We also received general feedback that the text on the interface should be larger. 

When asked how likely they would be to use the design again, we received an average of 3.8 out of 5. However, there was a single outlier who chose 1, with the rest picking either 4 or 5. The median was a 4 out of 5.

## Conclusions

The user testing we conducted for this last phase of development provided us with valuable feedback on both Chore No More’s functionality and visual design. Much of the feedback on the visual design was positive, with only a few minor suggestions on labeling and format. However, when it came to functionality, a few participants informed us about areas they felt needed improvement. For example, the assign chores menu was confusing for many users in its purpose and features. Additionally, there was a suggestion that the calendar page should allow days to be expanded. Overall, most users were happy with the site’s functionality and informed us they would be interested in using it again. Were more time devoted to this project, all issues would be quickly resolved. 

## Caveats

The biggest caveat in this phase of development was the limited set of participants we had for user testing. Chore No More is aiming to satisfy two primary demographics: parents with children and young adults with roommates. However, for this usability test, we only had access to the latter. This both skews the feedback we receive and prevents us from getting feedback on certain features related to adding child accounts. The other caveat during this phase was biased feedback when we completed the simulated user testing within our team. Because we are so close to the project we may not have found every possible issue which was present in the prototype.
